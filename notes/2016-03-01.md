#2016-02-23 Astro Statistics: Sections 3.3, 3.4

Attendance: Prof. Phillips, Ben Rose, Jason Wiggins, Christopher Wotta

<!-- Prof. Phillips, Aparna Bhattacharya, Geoffrey Lentner, Vini Placco, Ben Rose, Devin Whitten, Jason Wiggins, Christopher Wotta -->


**The uniform distribution** often appears in practical problems (e.g., the distribution of the position of a photon detected in a finite-size pixel)

**The Gaussian (normal) distribution** has two main properties that make it special. First, it lends itself to analytic treatment in many cases; most notably, a convolution of two Gaussian distributions is also Gaussian. Similarly, the Fourier transform of a Gaussian is also a Gaussian. Another unique feature of the Gaussian distribution is that the *sample mean and the sample variance are independent*. Second, the central limit theorem (see below) tells us that the mean of samples drawn from an almost arbitrary distribution will follow a Gaussian distribution. Hence, much of statistics and most classical results are based on an underlying assumption that a Gaussian distribution is applicable, although often it is not. **For example, the ubiquitous least-squares method for fitting linear models to data is invalid when the measurement errors do not follow a Gaussian distribution (see chapters 5 and 8).**

**The binomial distribution** describes the distribution of a variable that can take only two discrete values (say, 0 or 1, or success vs. failure, or an event happening or not). A common example of a process following a binomial distribution is the flipping of a coin.

**Poisson distribution** is a special case of the binomial distribution and thus it also describes the distribution of a discrete variable. (Remember: mu is the number of events per interval and k is the number of successes.) As μ increases, both the skewness and kurtosis decrease, and thus the Poisson distribution becomes more and more similar to a Gaussian distribution. Poisson must (1) have a known average and (2) occur independently of the time since (or location of) the last even. Poisson: The chance that the next event will be a success is 37% if it follows a Poisson distribution.

**Cauchy (Lorentzian) distribution**: We note that the ratio of two independent standard normal variables (z=(x -μ)/σ , with z drawn from N(0,1)) follows a Cauchy distribution with μ = 0 and γ = 1. Therefore, in cases when the quantity of interest is obtained as a ratio of two other measured quantities, assuming that it is distributed as a Gaussian is a really bad idea if the quantity in the denominator has a finite chance of taking on a zero value. For a general case of the ratio of two random variables drawn from two different Gaussian distributions, x = N(μ2,σ2)/N(μ1,σ1), the distribution of x is much more complex than the Cauchy distribution (it follows the so-called Hinkley distribution, which we will not discuss here). Given a set of measured x_i drawn from the Cauchy distribution, the location and scale parameters *cannot* be estimated by computing the mean value and standard deviation using standard expressions. To clarify, one can always compute a mean value for a set of numbers x_i, but this mean value will have a large scatter around μ, and furthermore, this scatter will not decrease with the sample size.

**Exponential (Laplace) distribution**: Like the Gaussian distribution, but pointy at the peak (not a smooth/differentiable function). The simplest case of a one-sided exponential distribution is p(x|τ) = τ^-1 exp(-x/τ), with both the expectation (mean) value and standard deviation equal to τ. This distribution describes the time between two successive events which occur continuously and independently at a constant rate (such as photons arriving at a detector); the number of such events during some fixed time interval, T, is given by the Poisson distribution with μ = T /τ (see eq. 3.52). Note that σ is larger than σ_Gaussian (σ ≈ 1.38*σ_Gaussian), and thus their comparison can be used to detect deviations from a Gaussian distribution toward an exponential distribution. While for the Gaussian distribution, |x_i - μ| > 5σ happens in fewer than one per million cases, for the exponential (Laplace) distribution it happens about once in a thousand cases.

**The χ^2 distribution**: The importance of the χ2 distribution will become apparent when discussing the maximum likelihood method in chapter 4.

**Student’s t distribution** is symmetric and bell shaped, but with heavier tails than for a Gaussian distribution. Tells about the difference between two data-based estimates (e.g., comparing the lengths of people's left feet with the lengths of their right feet.) The main difference (between the Student's t distribution and the χ^2 distribution) is that the definition of t is based on data-based estimates x and s, while the χ^2 statistic is based on *true* values μ and σ. Student’s t distribution also arises when comparing means of two samples. We discuss these results in more detail in chapter 5.

**Fisher’s F distribution** describes the distribution of the ratio of two independent χ^2_dof variables with d1 and d2 degrees of freedom, and is useful when comparing the standard deviations of two samples. Also, if x1 and x2 are two independent random variables drawn from the Cauchy distribution with location parameter μ, then the ratio |x1 - μ|/|x2 - μ| follows Fisher’s F distribution with d1 = d2 = 2.

The **beta distribution** is very useful in Bayesian analysis, as discussed in §5.6.2. In particular, the beta distribution is the conjugate prior for the binomial distribution (see §5.2.3).

**The gamma distribution** is useful in Bayesian statistics, as it is a conjugate prior to several distributions including the exponential (Laplace) distribution, and the Poisson distribution (see §5.2.3).

**The Weibull distribution**: The shape parameter k can be used to smoothly interpolate between the exponential distribution (corresponding to k = 1; see §3.3.6) and the Rayleigh distribution (k = 2; see §3.3.7). As k tends to infinity, the Weibull distribution morphs into a Dirac δ function. The Weibull distribution is often encountered in physics and engineering because it provides a good description of a random failure process with a variable rate, wind behavior, distribution of extreme values, and the size distribution of particles. For example, if x is the time to failure for a device with the failure rate proportional to a power of time, t^m, then x follows the Weibull distribution with k = m + 1. The distribution of the wind speed at a given location typically follows the Weibull distribution with k ≈ 2 (the k = 2 case arises when two components of a two-dimensional vector are uncorrelated and follow Gaussian distributions with equal variances).

SEE TABLE 3.1 FOR THE MEANS, STDEV, ETC. FOR VARIOUS DISTRIBUTIONS



We will read the following Sections for next week:

- 3.5.1 -- Christopher Wotta
- 3.5.2 -- Jason Wiggins
- 3.5.3 -- Ben Rose
- 3.5.4 -- Prof. Phillips

